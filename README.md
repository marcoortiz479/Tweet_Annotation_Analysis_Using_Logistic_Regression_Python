# Tweet Annotation Analysis using Logistic Regression

**Contributors**:
- Amberly Cavazos  
- Marco Ortiz 
- Chinmeri Nwagwu  
- Shutao Zhang

---

## ğŸ“ƒ Table of Contents

1. [Project Overview](#-project-overview)
2. [Files](#-files)
3. [Methodology](#-methodology)
    - [Preprocessing](#preprocessing)
    - [Feature Engineering](#feature-engineering)
    - [Modeling](#modeling)
4. [Results](#-results)
    - [Technology Classification](#technology-classification)
    - [Political Classification](#political-classification)
5. [Tools and Libraries](#-tools-and-libraries)
6. [Output](#-output)
7. [Future Improvements](#-future-improvements)
8. [Key Takeaways](#-key-takeaways)

---

## ğŸ“ Project Overview

This project aims to build classification models that analyze tweet content to determine whether it is related to **technology** or **politics**. Using annotated tweets, we applied machine learning techniques in Python to evaluate different types of features: lexical (TF-IDF), sentiment-based (TextBlob), and structural (punctuation, capitalization, etc.).

---

## ğŸ“ Files

- `Final_Annotated_Comments.xlsx` â€“ Annotated dataset used for modeling.
- `best_model_predictions_tech.csv` â€“ Results from the best model for technology classification.
- `best_model_predictions_pol.csv` â€“ Results from the best model for political classification.
- `final_project_modeling.ipynb` â€“ Jupyter Notebook with all code and outputs.

---

## ğŸ§ª Methodology

### Preprocessing

- Removed missing values in comment or label fields.
- Converted labels to binary:
  - `Technology`: `"tech"` â†’ 1, `"NoneTech"` â†’ 0  
  - `Political`: `"Pol"` â†’ 1, `"NoPol"` â†’ 0

### Feature Engineering

Three types of features were extracted:

| Feature Type | Description |
|--------------|-------------|
| **TF-IDF** | Term Frequencyâ€“Inverse Document Frequency matrix using top 1000 features |
| **Sentiment (Lexicon)** | Sentiment polarity and subjectivity via TextBlob |
| **Structural** | Count of capital letters, exclamation marks, and total text length |

### Modeling

- Classifier: **Logistic Regression**
- Evaluation: **Macro F1 Score** and **Micro F1 Score**
- Split: 80% training, 20% testing (stratified)

---

## ğŸ“Š Results

### Technology Classification

| Feature Set  | Macro F1 | Micro F1 |
|--------------|----------|----------|
| TF-IDF       | 0.5726   | 0.6650   |
| Lexicon      | 0.3865   | 0.6300   |
| Structural   | 0.3990   | 0.6300   |

ğŸ“‰ **Best Model**: **TF-IDF**

### Political Classification

| Feature Set  | Macro F1 | Micro F1 |
|--------------|----------|----------|
| TF-IDF       | 0.4872   | 0.9500   |
| Lexicon      | 0.4872   | 0.9500   |
| Structural   | 0.7217   | 0.9650   |

ğŸ“‰ **Best Model**: **Structural**

---

## ğŸ”§ Tools and Libraries

- **Python 3**
- `pandas`, `numpy`
- `sklearn` (Logistic Regression, metrics, train_test_split)
- `TextBlob` (sentiment analysis)
- `TfidfVectorizer` (feature extraction)

---

## ğŸ“ˆ Output

Final predictions for each best-performing model were saved in CSV format:
- `best_model_predictions_tech.csv`
- `best_model_predictions_pol.csv`

These contain:
- Original tweet (`comment`)
- True label (`true_label`)
- Predicted label (`predicted_label`)

---

## ğŸš€ Future Improvements

- Incorporate deep learning models (e.g., BERT) for context-aware classification.
- Use multi-label classification to allow tweets to be both political and technological.
- Visualize model performance using confusion matrices and ROC curves.

---

## ğŸ§  Key Takeaways

- Lexical features (TF-IDF) perform well for content-related classification.
- Structural cues are strong indicators of political content.
- Combining multiple feature types could potentially improve accuracy further.
